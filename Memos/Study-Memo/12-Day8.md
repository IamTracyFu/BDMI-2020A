## 第八周课程小结
今天学习了pandas的用法，并继续学习了机器智能的相关知识：
### Pandas基本用法
利用pandas生成表格，并查找表格中的行和列；
### 继续学习深度学习的基本知识
多层神经网络：如何确定这些网络的参数？\
复习Softmax函数、Simgoid函数，logistic函数……\
如何确定权重的优化参数？确定损失函数；权重初始化；反向传播；权重修正；
##### 损失函数
损失函数定义：衡量预测与预期的差异；\
引入度量函数D：平方和、均方差、交叉熵；\
训练的目的：通过调整网络的内部权重，使得在训练数据输入网络后，网络的实际输出与预期输出之间的差值最小化。\
损失函数的度量函数：对于回归任务，通过均方误差MSE来计算损失；对于分类任务，通过交叉熵CE来计算损失；\
利用python实现计算MSE与CE；
##### 反向传播算法
##### 权重更新——随机梯度下降法
损失函数Loss记为函数J(\theta),步长又称为学习率，一般为0.01;\
随机梯度下降法的核心：随机选取下一个样本；
##### 神经网络训练实际过程——小批量训练
将样本数据“分批训练”：优点是收敛性有保证，缺点是更新模型参数的时候需要遍历整个数据集；\
小批量训练是批量训练和随机梯度下降法的折中：\
1. 将整个训练集分成多个大小相同的子集，每个子集成为一个min-batch；\
2. 每个批次的数据被依次送入网络进行训练。训练完一个迷你批次，被称为一次迭代（iteration）；\
3. 训练集中所有训练样本都被送入网络，完成一次训练过程，成为一个时代（epoch）;\
神经网络的训练与推断：通过训练确定权重，应用推断结果可以对数据集之外的数据进行推断；
##### 实际示例
##### 用张量表示神经网络
张量的实质是N维数组
### TensorFlow 初步
TensorFlow2 有Constant, Placeholder, Variable三种表现形式；\
人工神经元Neuron模型
#### [课程代码](https://github.com/HuShiruo/BDMI-course/blob/main/W8_class_code.ipynb)
