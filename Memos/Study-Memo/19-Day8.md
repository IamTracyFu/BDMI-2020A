# WEEK8

[Schedule_week_8(点击连接即可拿到代码)](https://github.com/saturn-lab/BDMI-2020A/blob/master/Schedule/Part1/WW8/WW8-Plan.md)

pandas : 和 numpy 结合非常紧密的一个包

pandas 内置函数能完美对接数据库

注意转置、索引(index)、项目之间的差距



### 神经网络训练——权重参数的确定

1.确定损失函数

2.权重初始化

3.反向传播

4.权重修正

损失函数：交叉熵、MSE(和真实值时间的差值，并不是方差！记得去查课件)

反向传播算法

随机梯度下降法：就是随便抽取一个样本来进行计算(拟合).....

批训练：把整个数据集进行一次反向传播

小批量训练：整个数据集分解为子集，每个子集进行一次训练



迷你批次: 一个小批量训练过后出现的网络(训练就是修改一次网络参数)

随机梯度下降：迷你批次数量=1



在训练的时候注意把原始数据变为一个标准的正态分布 (运用方差和标准差进行归一化)



张量生成：np.random.randn()，m×n×p



深度的拟合能力是指数的增长，深度网络比广度网络有更好的拟合特性(但需要更长时间的训练和更复杂的梯度下降算法，可以参考playground.tensflow.org，)

[代码链接](https://github.com/Zoutianjian/The_first_Try_git/blob/master/Week_8.ipynb)