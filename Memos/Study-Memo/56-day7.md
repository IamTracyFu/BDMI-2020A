

## 第7节课

### numpy操作

```python
import numpy as np
c = np.array([[1,2,3],[4,5,6]])
d = np.array(((1,2,3),(4,5,6)))
e = np.array([(1,2,3),[4,5,6],(7,8,9)])
# [] ()没区别
```

运算:

```python
a*b
array([ 0,  5, 12, 21])
```

np.运算

```python
np.zeros((3,3))
np.arange(0,9).reshape(3,3)
np.ones((3,3))
np.dot(A,B)#矩阵乘法
```

数组变形:

reshape()改变形状  

ravel() 压平, 把矩阵压成一行 

transpose() 矩阵的转置, A^T^;

### 人工神经网络

人工神经元:

右边两张图片是经典的激活函数

### 人工神经元具体建模:

#### ReLU

#### 逻辑斯提



![img](https://qn-st0.yuketang.cn/Fgq5_GTURYNeKv1FapsQV9DzSBzT)

```python
def th(x):
    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))
def Logi_unit(x):
    w = np.array((-0.21, 0.3, 0.7))
    return th(np.dot(x,w))
x = np.array((1,0,1))
Logi_unit(x)

```

### 模拟布尔运算

使用逻辑斯提函数(sigmoid)

w1 w2为权重 b为偏置量

```python
def And(x):
    w = ((20,20))
    b = -30
    simga = sigmoid(np.dot(x,w) + b)
    return 1 if simga > 0.5 else 0
x = np.array((0,0))
And(x)
```

```python
def Or(x):
    w = np.array((20,20))
    b = -10
    simga = sigmoid(np.dot(x,w) + b)
    return int(np.rint(simga))
    #return 1 if simga > 0.5 else 0
x = np.array((0,0))
Or(x)
```

与非门:

```python
#NAND
def NAnd(x):
    w = np.array((-20,-20))
    b = 30
    simga = sigmoid(np.dot(x,w) + b)
    return int(np.rint(simga))
    #return 1 if simga > 0.5 else 0
x = np.array((1,1))
NAnd(x)
```

#### 二层网络:异或

XOR:

```python
#XOR
def Xor(x):
    y = np.array((Or(x),NAnd(x)))
    return And(y)
x = np.array((1,1))
Xor(x)
```



### 基于张量的神经网络

#### 分类任务-判断性别

准备数据集

进行二元分类(经验调参/手动调参)

![img](https://qn-st0.yuketang.cn/FnAjdApO-KQ7NTm7f1DuOL2zedb7)



**多层神经网络:**

#### softMax:归一化处理

```python
# softmax函数
w=np.array([1,2,3,4,5,6,7,8,9])
print(softmax)
def softmax(x):
    x_exp = [np.exp(i) for i in x]
    exp_sum = sum(x_exp)
    ans = [i/exp_sum for i in x_exp]
    return ans
softmax(w)
```

```python
def logit(x):
    return np.log(x/(1-x))
logit(0.9)
>>2.1972245773362196
```

#### 训练方法:

自动调参数方法->神经网络的训练

#### 损失函数

x1 为输入值, y1'为标签(预期输出), y1为实际输出

