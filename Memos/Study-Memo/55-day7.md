

## 第7节课

### numpy操作

```python
import numpy as np
c = np.array([[1,2,3],[4,5,6]])
d = np.array(((1,2,3),(4,5,6)))
e = np.array([(1,2,3),[4,5,6],(7,8,9)])
# [] ()没区别
```

运算:

```python
a*b
array([ 0,  5, 12, 21])
```

np.运算

```python
np.zeros((3,3))
np.arange(0,9).reshape(3,3)
np.ones((3,3))
np.dot(A,B)#矩阵乘法
```

数组变形:

reshape()改变形状  

ravel() 压平, 把矩阵压成一行 

transpose() 矩阵的转置, A^T^;

![image-20201027140825451](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\image-20201027140825451.png)

### 人工神经网络

人工神经元:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FtoLWQMdSIraKmXtJynWmCfmOG-0)

右边两张图片是经典的激活函数

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FnLZ2djNH8nA3l-1KHF_LSUWlvWh)

### 人工神经元具体建模:

#### ReLU

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FvdzkR64_Er5T8TZgsVuKrY9tcfu)

#### 逻辑斯提



![img](https://qn-st0.yuketang.cn/Fgq5_GTURYNeKv1FapsQV9DzSBzT)

```python
def th(x):
    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))
def Logi_unit(x):
    w = np.array((-0.21, 0.3, 0.7))
    return th(np.dot(x,w))
x = np.array((1,0,1))
Logi_unit(x)

```

### 模拟布尔运算

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Flesuh8bONjkhX-hfqFWmHueOiqB)

使用逻辑斯提函数(sigmoid)

特点: $\sigma(5) = 1 ;\sigma(5) = -1$![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FuKeHPBxnFokLOqmtpA6kjt_mX8l)

w1 w2为权重 b为偏置量

```python
def And(x):
    w = ((20,20))
    b = -30
    simga = sigmoid(np.dot(x,w) + b)
    return 1 if simga > 0.5 else 0
x = np.array((0,0))
And(x)
```

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fuo8YwRFpRPNkTUex9WBu0ol2UsO)

```python
def Or(x):
    w = np.array((20,20))
    b = -10
    simga = sigmoid(np.dot(x,w) + b)
    return int(np.rint(simga))
    #return 1 if simga > 0.5 else 0
x = np.array((0,0))
Or(x)
```

与非门:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FmuruxbdjzX4DRZQT_jR5dw0yxXg)

```python
#NAND
def NAnd(x):
    w = np.array((-20,-20))
    b = 30
    simga = sigmoid(np.dot(x,w) + b)
    return int(np.rint(simga))
    #return 1 if simga > 0.5 else 0
x = np.array((1,1))
NAnd(x)
```

#### 二层网络:异或

XOR:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FrgoztYkCX8QH34Q5IGqfwtIAvSp)

```python
#XOR
def Xor(x):
    y = np.array((Or(x),NAnd(x)))
    return And(y)
x = np.array((1,1))
Xor(x)
```



### 基于张量的神经网络

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FixBrujU-oiH3lu2D0sl_3uwdw1-)

#### 分类任务-判断性别

准备数据集

进行二元分类(经验调参/手动调参)

![img](https://qn-st0.yuketang.cn/FnAjdApO-KQ7NTm7f1DuOL2zedb7)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fh4-todz1DbuW0i7Z4RnbRS9-0yi)

**多层神经网络:**

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FvYNCxao3KIjOiQM5hFRE_3gzFam)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FkSm0XZZnI4dR0oh8Jfo3Tzygtqc)

#### softMax:归一化处理

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FlXv7pZ2wUA1A9rMIAY26YMhbHCX)

```python
# softmax函数
w=np.array([1,2,3,4,5,6,7,8,9])
print(softmax)
def softmax(x):
    x_exp = [np.exp(i) for i in x]
    exp_sum = sum(x_exp)
    ans = [i/exp_sum for i in x_exp]
    return ans
softmax(w)
```

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fl-EtQN6ecalydRvCMeeXXrMgeTy)

```python
def logit(x):
    return np.log(x/(1-x))
logit(0.9)
>>2.1972245773362196
```

#### 训练方法:

自动调参数方法->神经网络的训练

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fo4pE3QbO0rVpSvOQqYkqPewAbOk)

#### 损失函数

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fp1XfpdWVo03mzucIwr8VqVadrEE)![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fl-SxGUyhQITHpdl4v4K-vea_4S5)

x1 为输入值, y1'为标签(预期输出), y1为实际输出

