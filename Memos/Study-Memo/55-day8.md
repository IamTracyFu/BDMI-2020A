## 第8节课

使用pandas Tenso

### pandas

#### 一维: Series

index: 索引

#### 二维: DataFrame

![img](https://qn-st0.yuketang.cn/FgPwcCMYRxt3U5u3nYwlPh3xuP3J)



![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FvlEhVkRTu4RwnyuyJH7wYIapcFP)

代码示例:

```python
classmates = {
    "zhangsan":pd.Series(['2019010001','male'], index=['ID','Sex']),
    "wangwu":pd.Series(['2019014561','female'],index=['ID','Sex'])
}
#生成数据表格
df_class = pd.DataFrame(classmates)
#进行转置
df_class = df_class.T
#查询
df_class.loc['zhangsan']
df_class["Sex"]
```

用途: 读取各种数据很方便

### TensorFlow

分类 回归预测

用途示例: 判断性别

问题: ==如何找到合适的权重===> 经验调参法

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fgpqc_q-0NEGyxT7Y1HPWfdPZFyT)

三个输入特征, 两个输出项=>打分值

**15**个权重

softmax:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FlXv7pZ2wUA1A9rMIAY26YMhbHCX)

分对数: sigmoid函数的反函数

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fl-EtQN6ecalydRvCMeeXXrMgeTy)

```python
def logit(x):
    return np.log(x/(1-x))
logit(0.9)
>>2.1972245773362196
```

#### 如何搭建更多层数的网络

有没有自动化确定权重的方法

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FlFTH_P0Wjwld_ZgZCQhjsB9iNfq)

#### 损失函数

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FodXBcLPiwtrJMFbw-E0pzLN1JZ4)

利用度量函数衡量差异, 通过调整内部权重, 使得差异最小化

均方误差=>各个数据偏离真实值的差值的平方和的平均数

#### 交叉熵

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FrQ51N7RTPb9FaUfUeNB4tuE2vxQ)

```python
import numpy as np
# 交叉熵
def CE(label,real): 
    return np.sum(-label*np.log(real))
```

==损失函数==

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FgE2XLf0JtqwNJjIX7OOBjKP1jSF)

#### 倒数与梯度

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FoqNwwsbe6wilUEjhJqmOyTtBcjV)

#### 权重更新=>随机梯度下降法

梯度算子=>

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fs9Z-b8Rg43-MogkKXNgReP-_RXY)

随机梯度下降法:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FhhG8IE2Ee4I1RwFsdzA0Cw0DGgp)

批量训练法(batch): 使用整个数据集进行训练

优点: 收敛度有保障

缺点: 训练量太大



小批量训练: (mini-batch)

训练完一个迷你批次, 则称为一个迭代; 

一个时代是指训练集中所有训练样本都被送入网络, 完成一次训练的过程.

#### 神经网络的训练和推断

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FovD1bsZ6BMnQqs9xyBcfqwX2y6I)

### 逻辑斯蒂回归实践

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FnEwMCLv4ZVLMc9MjZQ-CMmhhRX2)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Flzkcri4VaxKA_t6fhygJlee4yfO)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FgGfkYaBVM4l3sz3JnKJ6brjPPda)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fpk4eZthDP1w2OC31iCVgY6IJ4Mq)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FlCJCnZ8fa4u17lxGO_R9-6FMNC9)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FmutEmT4zzVUTWJncoUzpZ2D0iqY)

对数似然:

```python
def log_likelihood(features, target, weights):
    scores = np.dot(features, weights)
    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )
    return ll
```



![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fud4cqZseikgIznTGT0Vu4SZ0PRY)



代码实现:

```python
def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):
    if add_intercept:
        intercept = np.ones((features.shape[0], 1))
        features = np.hstack((intercept, features))
        
    weights = np.zeros(features.shape[1])
    
    for step in range(num_steps):
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # Update weights with log likelihood gradient
        output_error_signal = target - predictions
        
        gradient = np.dot(features.T, output_error_signal)
        weights += learning_rate * gradient

        # Print log-likelihood every so often
        if step % 10000 == 0:
            print(log_likelihood(features, target, weights))
        
    return weights
```

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FlLWi0Tb-X2uIPafIOpGk7VW8WPv)

输入示例:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fm9h-DP75vgoJ9wibt51Oec61W4v)

#### 自动微分

用计算机算误差的微分值?

(标准化的神经网络搭建=>简化了神经网络搭建)

### TensorFlow=> 用张量表示神经网络

Tensor张量=> 实质是N维数组

1维=> 向量, 2维=> 矩阵, 高维=> 矩阵组成的向量

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FqjjDozbbfvJI2uafba14ftVXFjf)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FtEv7qlLDvSvDqfztMqK--XNZA9I)

每个像素点=> (x,y,颜色)

颜色=>RGB

#### 用张量来表示神经网络

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FpRgoIpKCW2-swQFyp7oVmikrAur)

每一层都可以表示成张量, 最终得到的也是张量

#### 用张量表示图像

图像是像素的二维张量, 每个像素也是张量:laughing:

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fmg2bj-f89AEqE-LTvpu6l9Rw0py)

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FomTqZs5tuQEjGFN5vuCch_j6c3A)

### TensorFlow基础

![image-20201103154848762](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\image-20201103154848762.png)

Neuron: 人工神经元模型=> Network: 网络

重要的是损失函数和梯度下降

#### TensorFlow 2 预备知识

计算图(Graph) 即时执行(Eager Execution)

Tensor: Constant Placeholder Variable 三种形式

​				reshape rank

#### 人工神经元Neuron

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\Fmm3cVQaBKX-Szd9ljCi8aJAadFC)

左边: 线性的加权叠加

中间: 非线性操作

#### Loss 与 GD

![img](C:\Users\75451\Desktop\learning\大数据与机器智能\pic\FsVSbv-PRo3tPtMZW6LBnugB7AHh)

#### 网络的宽度 深度

增加宽度与深度

