# 深度学习2
## pandas
可以实现numpy的功能

#### 利用字典来进行列的输入

```python
d = {'姓名' : pd.Series(['帅周','杨哥','王酷'],index=['2019','2017','2018']),
     '分数' : pd.Series([4.,3.9,3.9],index=['2019','2017','2018'])} #后面index是索引，接下来会将两列合并,如果不对应，会自动填充NaN
df = pd.DataFrame(d)
```
#### 表格的查询与转置
```python
df.loc['2019']  ##按编号查询
df.iloc[0]  ## 按顺序查询
df=df.T #转置表格
```
#### 表格的输入输出
```python
df.to_csv('foo.csv')
pd.read_csv('foo.csv')
```
## 深度学习
#### 经验法则
输入结果与某个因素越相关就权重越大。
#### 多层网络
输入层——隐藏层——输出层
#### 自动化权重确定
1. 确定损失函数
   平方和，均方差、交叉熵
      均方误差MSE，交叉熵CE
#####                    误差最小化
2. 初始权重
3. 反向传播
4. 权重修正
#### 均方误差MSE
$$
MSE=\sum(x-mean(x))^2/n
$$

代码：

```python
def MSE(a):
    return np.sum((a-np.mean(a))**2)/len(a)
```
#### 交叉熵
$$
H_y(y)=-\sum y_i*log(y_i)
$$

代码：
```python
def CE(a,b):  
    return np.sum(-a*np.log(b))
```

### 反传算法
** 分类用交叉熵，计算用MSE **
导数与梯度：
** 梯度下降算法**
找到局部最小值。
链式求导法则，反向计算损失函数对梯度的权重。
** 权重更新**
** 随机梯度下降法——样本随机**
$$
\theta =\theta-\eta*\nabla J(\theta)
$$
### 整个数据集batch批量训练
缺点，计算量较大
** mini 批次训练 **
分成小组，每批训练（iteration迭代），一个时代（都训练完了epoch）
### 过程
训练与推断

#### 用逻辑斯提回归进行分类
```python
data['label'] = data['label'].apply(lambda x : {'男': 0, '女': 1}[x])#更换标签
```
######  激活函数
最大似然函数
$$
L=\sum (1-p)^{1-y}*p^y
$$
计算可能的权重参数！如果对数似然最大，所用的参数就是对的。

## 张量表示神经网络
### 人工神经元模型
宽度VS深度