### 第七节课：深度学习初步

#### Python库：numpy

numpy里面可以支持很多线代的运算，比如定义矩阵： 

```python
import numpy as np
c = np.array([[1,2,3],[4,5,6]])
c = np.array([['a','b'],['c','d']],dtype='<U1')
print(c)

d = np.array([[1,2],[3,4]],dtype = complex)			#dtype表示数据格式，complex表示复数，U表示固定长度的unicode类型
print(d)
```

数组对象之间的运算符仅作用于对应的元素，即一一对应的元素进行乘积、相加等操作，对开根和正弦运算也是同理。

以下是一些np里自带操作的整理：

| np.ones(3,3)    | 3x3的全1矩阵     |
| --------------- | ---------------- |
| np.dot(A,B)     | 矩阵乘积         |
| a.reshape=(3,4) | 改变数组为3行4列 |
| a.ravel()       | 把其变为一维数组 |
| a.transpose()   | 转置             |
| np.identity()   | 单位矩阵         |
| array.shape()   | 得到对象的形状   |
| array.dtype()   | 得到变量类型     |

import pdb;pdb.set_trace()：设置断点（这个vscode自带有断点功能，不是必须的）



scipy库也有与numpy库相关的一些功能：

| scipy.linalg.inv()       | 求逆     |
| ------------------------ | -------- |
| scipy.linalg.eig()       | 求特征值 |
| scipy.spatial.distance() | 距离计算 |



### 机器学习1

##### 人工神经元（带权重的函数）

Def. 一组输入的线性加权叠加，经过一个非线性变换进行输出$y=f(\Sigma W_iX_i+b)$

激活函数：指这个非线性变换

最简单的激活函数：

(Sigmoid函数):$\frac{1}{1+e^{-x}}$(Logistic Regression函数)输入5的时候，输出1；输入-5的时候，输出0

(tanh函数):$\frac{e^x-e^{-x}}{e^x+e^{-x}}$

(RelU函数):max(x,0)(整流神经元)

##### 单个人工神经元能力-模拟布尔运算

与门：w=[20,20],b=-30（偏置量）,然后扔到逻辑斯提里。这样$\sigma(-30)$,$\sigma(-10)$,$\sigma(10)$,由于$\sigma(5)\approx1$,$\sigma(-5)\approx-1$，因而做到了与门。

```python
def Logistic_ANDunit(x):
    b = -30
    w=np.array([20,20])
    
    add = np.dot(x,w)+b
    return 1/(1+np.exp(-add))

x1=np.array([0,0])
print(Logistic_ANDunit(x1))
```

如此可以模拟逻辑门进行布尔运算。



或门：把偏置量b=-10(本质上也是把真值表的输出量做了一个分类，一个0，三个1)

```python
def Logistic_ORunit(x):
    b = -10
    w=np.array([20,20])
    
    add = np.dot(x,w)+b
    return 1/(1+np.exp(-add))

x1=np.array([0,0])
print(Logistic_ORunit(x1))
```



非门：权重w=-20(负相关),b=10。

与非门：b=30,w=[-20,-20]

```python
def Logistic_NANDunit(x):
    b = 30
    w=np.array([-20,-20])
    
    add = np.dot(x,w)+b
    return 1/(1+np.exp(-add))

x1=np.array([0,0])
print(Logistic_NANDunit(x1))
```

##### 多个神经元,解决异或运算

然而对异或运算，不是线性分类问题（无法用直线分割成两个部分）

需要用多个神经元。

（或和与非的门）输入到与的门



### 机器学习2

人工神经元：门电路（类脑计算）、基于张量的人工神经元

#### 神经网络的用途

神经网络是一个很好的数学模型，属于机器学习的监督学习方法。

多层的神经网络又称为深度学习，主要用于解决两类任务：

1、分类任务 （分类方法）：（通过数据集的学习，对新数值进行集合分类）

2、预测任务（回归方法）：通过对数据集的学习，预测新的数值



可以手动调整权重参数：

判断输入与结果是正相关还是负相关，确定权重W是正还是负

判断输入与结果的重要程度，越重要，权重越大。



显然，网络层数增加可以提升能力，但要调的参数更多了。

多层神经网络：实例：输入层，隐藏层，输出层。



在输出层，可以将输出的值相加认为是1，即进行归一化。

softmax():将所有的输出进行归一化，为$\frac{e^{z_m}}{\Sigma e^{z_i}}$

分对数(logit)，sigmoid的反函数$\sigma^{-1}(x)=log(\frac{x}{1-x})$



但层数一多就不可能再一个个手动调了，这是就要自动调参，即神经网络的训练。

一般的流程：

1)确定损失函数(Loss)

2)权重初始化(initialization)

3)反向传播（往反的方向修正）(Back Propagation)

4)权重修正(weights Adjusting)



神经网络方法属于机器学习的监督学习方法，特点是根据训练数据集进行学习，然后推断新的实例。

训练数据集由样本集组成，每个样本上都有相应的标签(label)，用来指导训练过程。



E.a. 采用带有标签的训练样本对神经网络进行学习，确定网络的权重参数

如：数据集$(x_1,y_1')$，而实际输出为$(x_1,y_1)$,$y_1'$是期望的值，$y_1$是实际的值，因而有差异$(y_1,y_1')$

引入度量函数度量Y与Y'的差异，训练的目标在于调整参数后能使网络的实际输出和预期输出一致，即往距离小的方向调整。



对于回归任务，通常采用均方误差(方差，标准差，均方误差，均方根误差……)；对于分类任务，采用交叉熵的公式。($H_{y'}(y)=-\Sigma y_i'log(y_i)$)