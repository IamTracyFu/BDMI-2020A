# [练习链接](https://github.com/wsp1911/BDMI_class_practice/tree/master/8)

# pandas

## Series：一维数据

```python
s = pd.Series(np.random.randn(5), index=['a','b','c','d','e'])
s + s
s * 2
# Series默认用index进行对齐而非相对位置，不能对齐的部分当作缺失值处理
s[1:] + s[:-1] 
```

## DataFrame：二维数据

```python
DataFrame(data, index, columns)
# 从字典构造
data = {"id":pd.Series(["2017010807","2017010870"],index=["wsp","ztj"]),
        "gender":pd.Series(["male","female"],index=["wsp","ztj"])}
data = pd.DataFrame(data)
# 列选
data["id"]
# 用label行选
data.loc["wsp"]
# 用整数索引行选
data.iloc[0]
# 行切片
data[0:2]
# 用布尔序列进行行选
data[[True,False]]
```

# 多层神经网络

## 损失函数

### MSE

误差平方和的平均数，主要用于回归

$MSE(x,r)=\frac{1}{N}\sum_{n=0}^{N-1}(x_n-r_n)^2$

r为x的真值

### 交叉熵

负对数似然损失函数，主要用于分类任务

$H_{y^{'}}(y)=-\sum_iy_i^{'}{\rm log}(y_i)$

其中y'为训练样本对应的标签，y代表神经网络的输出

输出与标签越接近，交叉熵越小

## 反向传播算法

### 梯度下降法

由函数上当前点对应梯度的反方向，按照一定步长进行迭代搜索

### 反向传播

根据损失函数的性质和链式求导法则，反向逐层计算损失函数对权重的梯度（各个偏导数）

### 权重更新

学习率过大会引起震荡，学习率太小收敛太慢

#### 随机梯度下降法

* 随机初始化权重和偏差
* 选取一个随机样本
* 根据网络输出，从最后一层开始，逐层计算每层权重的偏导数
* 调整权值
* 继续随机选取下一样本

#### 实际训练过程

将样本数据“分批训练”，可保证收敛，但需要遍历整个数据集

batch：整个训练集

mini-batch：batch的子集，大小为mini-batch_size

iteration：训练一个mini-batch，更新一次权重

epoch：所有训练样本都被送入网络，完成一次训练的过程

# Logistic回归

似然函数：

$\displaystyle L(\beta_0,\beta)=\prod_{i=1}^np(x_i)^{y_i}(1-p(x_i))^{1-y_i}$

$ll=\sum_{i=1}^Ny_i{\beta^Tx_i-{\rm log}(1+e^{\beta^Tx_i})}$

$\frac{\part l}{\part \beta_i}=\sum_{i=1}^N{(y_i-p(x_i;\beta_0,\beta))x_{ij}}$

$\nabla ll=X^T(Y-Predictions)$

# TensorFlow

## Tensor

Constant, Placeholder, Variable

reshape, rank